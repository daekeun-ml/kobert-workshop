{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying fine-tuned model to SageMaker Endpoint to perform Inference (Script Mode)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Amazon SageMaker manages the built-in deep learning frameworks, including MXNet, TensorFlow, PyTorch, and Chainer, as a managed Docker container, so you can easily train the model, or deploy to the endpoint using script without building a new Docker container with BYOC(Bring Your Own Container).\n",
    "\n",
    "There are several ways to include dependency packages in a script.\n",
    "\n",
    "- ***Option 1.*** Insert `requirements.txt` in the directory containing the entry point script. However, as of May 2020 TensorFlow framework does not support `requirements.txt` when setting script mode, so please use another option if you use TensorFlow.\n",
    "- ***Option 2.*** Include a dependency package installation command in the inference script. For example,\n",
    "\n",
    "```python\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', 'gluonnlp', 'torch', 'sentencepiece', \n",
    "                'onnxruntime', 'transformers', 'git+https://git@github.com/SKTBrain/KoBERT.git@master'])\n",
    "\n",
    "```\n",
    "\n",
    "Note that the endpoint deployment time is about 9-11 minutes when using the GPU instance and about 7-8 minutes when using the CPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "#model_data = 's3://<YOUR BUCKET>/<YOUR MODEL PATH>/model.tar.gz'\n",
    "model_data = 's3://sagemaker-us-east-1-143656149352/mxnet-training-2020-05-19-01-08-59-727/output/model.tar.gz'\n",
    "\n",
    "mxnet_model = MXNetModel(model_data=model_data,\n",
    "                         role=role,\n",
    "                         entry_point='inference.py',\n",
    "                         source_dir = './src',\n",
    "                         py_version='py3',\n",
    "                         framework_version='1.6.0'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!mxnet-inference-2020-05-28-14-36-38-048\n",
      "CPU times: user 31.7 s, sys: 2.63 s, total: 34.3 s\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = mxnet_model.deploy(instance_type='ml.p2.xlarge', initial_instance_count=1)\n",
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the endpoint is created and you want to restart the jupyter notebook, initializing the predictor can be done using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# from sagemaker.mxnet.model import MXNetPredictor\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# endpoint_name = '<YOUR ENDPOINT NAME>'\n",
    "# predictor = MXNetPredictor(endpoint_name, sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below performs real-time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': [0.03050542250275612, 0.9694945812225342], 'time': 0.01893901824951172}\n"
     ]
    }
   ],
   "source": [
    "# Wow, this is a story that repeats reversal over reversal. Highly recommended\n",
    "input_sentence = '우와, 정말 반전에 반전을 거듭하는 스토리입니다. 강력 추천합니다.'\n",
    "pred_out = predictor.predict(input_sentence)\n",
    "print(pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': [0.9753417372703552, 0.02465830184519291], 'time': 0.019909381866455078}\n"
     ]
    }
   ],
   "source": [
    "# The contents are really messed up, and the actor's acting skills are also messed up.\n",
    "input_sentence = '하하, 정말 엉망진창에 배우 연기력도 꽝이에요.'\n",
    "pred_out = predictor.predict(input_sentence)\n",
    "print(pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()\n",
    "# predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
