{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying fine-tuned model to SageMaker Endpoint to perform Inference (Bring Your Own Container)\n",
    "\n",
    "---\n",
    "\n",
    "A great tutorial has already been introduced in the AWS Korea AIML blog and GitHub by Amazon Machine Learning Solutions Lab. Based on this method, it is easy to perform endpoint deployment by making minor modifications.\n",
    "- https://github.com/aws-samples/kogpt2-sagemaker/blob/master/sagemaker-deploy-en.md\n",
    "\n",
    "\n",
    "### Modify DockerFile\n",
    "\n",
    "Basic contents can be done in the same way as the above tutorial. When editing Dockerfile(Based on `./docker/1.6.0/py3/Dockerfile.gpu`), you need to edit as follows. (If you do not use KoGPT2, you can delete 4 lines below `#For KoGPT2 installation`.)\n",
    "\n",
    "```shell\n",
    "RUN ${PIP} install --no-cache-dir \\\n",
    "    ${MX_URL} \\\n",
    "    git+git://github.com/dmlc/gluon-nlp.git@v0.9.0 \\\n",
    "    gluoncv==0.6.0 \\\n",
    "    mxnet-model-server==$MMS_VERSION \\\n",
    "    keras-mxnet==2.2.4.1 \\\n",
    "    numpy==1.17.4 \\\n",
    "    onnx==1.4.1 \\\n",
    "    \"sagemaker-mxnet-inferenc>2\"\n",
    "\n",
    "# For KoBERT installation\n",
    "RUN git clone https://github.com/SKTBrain/KoBERT.git \\\n",
    "&& cd KoBERT \\\n",
    "&& ${PIP} install -r requirements.txt \\\n",
    "&& ${PIP} install .\n",
    "\n",
    "# For KoGPT2 installation\n",
    "RUN git clone https://github.com/SKT-AI/KoGPT2.git \\\n",
    "&& cd KoGPT2 \\\n",
    "&& ${PIP} install -r requirements.txt \\\n",
    "&& ${PIP} install .\n",
    "\n",
    "RUN ${PIP} uninstall -y mxnet ${MX_URL}\n",
    "RUN ${PIP} install ${MX_URL}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Now you can paste the script code below in the SageMaker notebook instance and then create the endpoint by specifying the script code as the entrypoint. The code example is shown below.\n",
    "\n",
    "Note that the endpoint deployment time is about 9-11 minutes when using the GPU instance and about 7-8 minutes when using the CPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "#model_data = 's3://<YOUR BUCKET>/<YOUR MODEL PATH>/model.tar.gz'\n",
    "model_data = 's3://sagemaker-us-east-1-143656149352/mxnet-training-2020-05-19-01-08-59-727/output/model.tar.gz'\n",
    "#entry_point = './src/inference.py'\n",
    "\n",
    "mxnet_model = MXNetModel(model_data=model_data,\n",
    "                         role=role,\n",
    "                         entry_point='inference.py',\n",
    "                         source_dir = './src',\n",
    "                         py_version='py3',\n",
    "                         framework_version='1.6.0'\n",
    "                         #image='<YOUR ECR IMAGE>'\n",
    "                         #model_server_workers=2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = mxnet_model.deploy(instance_type='ml.p2.xlarge', initial_instance_count=1)\n",
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!konlp-2020-05-14-11-57-34-075\n",
      "CPU times: user 23 s, sys: 3.89 s, total: 26.9 s\n",
      "Wall time: 9min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = mxnet_model.deploy(instance_type='ml.p2.xlarge', initial_instance_count=1)\n",
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the endpoint is created and you want to restart the jupyter notebook, initializing the predictor can be done using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# from sagemaker.mxnet.model import MXNetPredictor\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# endpoint_name = '<YOUR ENDPOINT NAME>'\n",
    "# predictor = MXNetPredictor(endpoint_name, sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below performs real-time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-4z4hp_1  |\u001b[0m 2020-05-28 11:51:15,497 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 91\r\n",
      "\u001b[36malgo-1-4z4hp_1  |\u001b[0m 2020-05-28 11:51:15,497 [INFO ] W-9002-model ACCESS_LOG - /172.18.0.1:54172 \"POST /invocations HTTP/1.1\" 200 94\r\n",
      "{'score': [0.030505415052175522, 0.9694945812225342], 'time': 0.08880257606506348}\n"
     ]
    }
   ],
   "source": [
    "# Wow, this is a story that repeats reversal over reversal. Highly recommended\n",
    "input_sentence = '우와, 정말 반전에 반전을 거듭하는 스토리입니다. 강력 추천합니다.'\n",
    "pred_out = predictor.predict(input_sentence)\n",
    "print(pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-4z4hp_1  |\u001b[0m 2020-05-28 11:51:21,400 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 87\r\n",
      "\u001b[36malgo-1-4z4hp_1  |\u001b[0m 2020-05-28 11:51:21,401 [INFO ] W-9003-model ACCESS_LOG - /172.18.0.1:54172 \"POST /invocations HTTP/1.1\" 200 89\r\n",
      "{'score': [0.9753417372703552, 0.024658288806676865], 'time': 0.08612608909606934}\n"
     ]
    }
   ],
   "source": [
    "# The contents are really messed up, and the actor's acting skills are also messed up.\n",
    "input_sentence = '하하, 정말 엉망진창에 배우 연기력도 꽝이에요.'\n",
    "pred_out = predictor.predict(input_sentence)\n",
    "print(pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional. Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()\n",
    "# predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
